---
title: "Introdução ao Deep Learning com R"
author: "Daniel Falbel"
institute: "RStudio, Inc."
date: "2020-01-28 (RStudio conf)"
output:
  xaringan::moon_reader:
    css: ["default", "theme.css"]
    lib_dir: libs
    seal: false
    nature:
      beforeInit: "macro.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

class: middle, center, inverse

# Introdução ao Deep Learning com R

### Curso-R

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, 
                      dpi = 96, fig.height = 4.7, 
                      fig.width = 10, fig.retina = 3)
library(tidyverse)
```

---
## Nesse curso vamos falar de

TODO

---

## O que é Deep learning?

-   Subconjunto de técnicas de Machine Learning

![:scale 50%](img/artificial-intelligence.png)

Fonte: Deep Learning with R, Chollet, F et al

---
## O que é Deep learning?

- Aprender representações dos dados em camadas. Aprender representações dos dados 'hierarquicamente'. 

![:scale 75%](img/dl-deep.png)

Fonte: Deep Learning with R, Chollet, F et al

---

## Por que Deep Learning

-   Estado da arte em diversos problemas muito importantes.

![:scale 65%](img/Screen Shot 2020-05-09 at 10.51.43.png)

Fonte: [Apr. resultados ILSVRC 2017](http://image-net.org/challenges/talks_2017/ILSVRC2017_overview.pdf)

---

## Por que Deep learning?

- Resultados impressionantes em tradução.

![](img/image00.png)

Fonte: [A Neural Network for Machine Translation, at Production Scale](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)
---

## Por que Deep Learning?

- Resultados em Reinforcement Learning

![:scale 75%](img/unnamed.gif)

Fonte: [AlphaZero: Shedding new light on chess, shogi, and Go](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go)

---

# Por que Deep Learning?

- Reconhecimento de fala
- Reconhecimento em vídeos

![:scale 50%](img/1_vAhhl1cMGTzBUTYnIMssIQ.png)

Fonte: [Tesla’s Deep Learning at Scale: Using Billions of Miles to Train Neural Networks](https://towardsdatascience.com/teslas-deep-learning-at-scale-7eed85b235d3)

---

class: middle, center, inverse

# Tudo começa com ....

## Regressão Linear

---

## Regressão Linear

Prevendo o preço de uma casa a partir do tamanho em metros quadrados.

```{r}
df <- tibble(
  m2 = rgamma(n = 100, scale = 5, shape = 20),
  preco = 100000 + (6200 + rnorm(n = 100, sd = 800)) * m2 
)

ggplot(df, aes(x = m2, y = preco)) +
  geom_point() +
  coord_cartesian(ylim = c(0, NA)) +
  scale_y_continuous(
    name = "Preço (em R$)",
    labels = scales::dollar_format(prefix = "R$", big.mark = ".", decimal.mark = ",")
    
    ) +
  scale_x_continuous(name = "Área (em mˆ2)") +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x)
```

---

## Definição do modelo

Definimos o modelo de regressão linear da seguinte forma:

$$\hat{y_i} = w \times x_i + b$$
- $y_i$: preço do imóvel $i$
- $x_i$: área do imóvel $i$

Poderíamos escrever

$$\hat{y_i} = f(x_i)$$

em que:

- $f(x) = w \times x + b$

Chamamos $f$ de **'layer'** (camada) na linguagem do Deep learning.

---

## Layers (Camadas)

- Uma **'layer'** é uma transformação dos dados que é
parametrizada por **pesos**. 

- 'Aprender', então, significa encontrar os melhores **pesos** para cada camada.

No exemplo:

$$f(x) = w \times x + b$$

Os pesos são $w$ e $b$.

- Essas camadas são os 'tijolos' do Deep Learning e existem diversas 'camadas'.

- A camada do exemplo é chamada de **'Densa'** ou **'Linear'**.

- Um modelo pode possuir uma ou mais dessas camadas.

---

# Regressão Linear

![:scale 65%](img/Screen Shot 2020-05-09 at 14.32.20.png)

Fonte: Figura adaptada do Deep Learning with R, Chollet, F. et al.


**Objetivo**: encontrar os melhores 'pesos' para essa Layer.

---

## Função de perda

- Mede quanto o modelo está perto do que queremos que ele fique.

- No nosso caso, mede o quanto a previsão dada por $w \times x + b$ está perto de $y$, o verdadeiro valor daquele imóvel.

- Uma função de perda bastante usada é o **MSE** - Erro quadrático médio.

- O MSE é dado por:

$$L(\hat{y}) = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y_i})^2$$
- Podemos reescrever em função dos pesos:

$$L(w, b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - w \times x_i - b)^2$$
---

## Função de perda em função do valor dos parâmetros

```{r}
mse <- function(w, b) {
  mean((df$preco - (df$m2*w + b))^2)
}

expand.grid(
  w = seq(from = -10000, to = 10000, length.out = 300),
  b = seq(from = -200000, to = 200000, length.out = 100)
) %>% 
  rowwise() %>% 
  mutate(mse = mse(w, b)) %>% 
  ggplot(aes(x = w, y = b, fill = mse)) +
  geom_bin2d(stat = "identity") +
  scale_fill_gradient(high = "blue", low = "white", trans = "log") +
  theme_minimal() +
  geom_point(
    data = data.frame(w = 6200, b = 100000),
    aes(x = w, y = b, fill = NULL),
    color = "red",
    size = 5
  )
```

---

## Função de perda

![:scale 50%](img/Screen Shot 2020-05-09 at 18.30.44.png)

Fonte: Figura adaptada do Deep Learning with R, Chollet, F. et al.

- A **função de perda** nos diz, dado um valor para cada peso, o quão próximo estamos do valor esperado.

---

## Encontrando o mínimo da função de perda

Até agora:

- Vimos que nosso objetivo é minimizar a função de perda.
- Para isso precisamos encontrar o valor dos pesos que minimiza faz a função de perda ter o valor mínimo possível.

Agora:

- O processo de encontrar o mínimo de uma função é chamado de **otimização**. 
- Existem diversos algoritmos de otimização. Em geral eles são adequados ou não dependendo da função que você está otimizando.
- Em Deep Learning usamos algoritmos que são variações do **Gradient Descent** - método de descida do gradiente.

---

## Gradient Descent 

O **gradient descent** diz que se uma função $L(x)$ é diferenciável na vizinhança de um ponto $w$ então $L(x)$ decresce mais rapidamente se você andar de $w$ para uma direção contrária ao gradient de $L$ no ponto $w$. 

Em outras palavras, fazer $w - \nabla L(w)$ é a forma de caminhar o mais rápido possível para o mínimo de $L(x)$.

![:scale 20%](img/700px-Gradient_descent.svg.png)

Fonte: [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)

---

## Gradient descent no exemplo

No nosso exemplo temos: 

$$L(w, b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - w \times x_i - b)^2$$

Então para andar mais rápido para o mínimo de $L$ cada passo de $w$ e $b$ tem que ser calculado da seguinte forma:

$$w_{(i+1)} = w_{(i)} - \frac{\partial L}{\partial w} = w_{(i)} - \frac{1}{n}\sum_{i=1}^{n} (-2 \times x_i)(y_i - w \times x_i -b)$$

$$b_{(i+1)} = b_{(i)} - \frac{\partial L}{\partial b} = b_{(i)} - \frac{1}{n} \sum_{i=1}^{n} (-2)(y_i - w \times x_i -b)$$

**Calma!** vamos dissecar essas fórmulas.

---


## Otimizando

Esse é o diagrama geral que vale para os modelos que vamos implementar neste curso.

![:scale 50%](img/Screen Shot 2020-05-09 at 19.17.36.png)

Fonte: Figura adaptada do Deep Learning with R, Chollet, F. et al.

---

## Grafo de computação

TODO: fazer na mão

---

## Grafo de computação

TODO: colocar as contas

---

Fazer no R - exemplo 01

---

## SGD (Stochastic gradient descent)

---

Fazer no R - exemplo 02

---

## TensorFlow

TODO

---

## Keras

---

Fazer no R - exemplo 03