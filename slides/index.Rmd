---
title: "Introdução ao Deep Learning com R"
author: "Daniel Falbel"
institute: "RStudio, Inc."
date: "2020-01-28 (RStudio conf)"
output:
  xaringan::moon_reader:
    css: ["default", "theme.css"]
    lib_dir: libs
    seal: false
    nature:
      beforeInit: "macro.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

class: middle, center, inverse

# Introdução ao Deep Learning com R

### Curso-R

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, 
                      dpi = 96, fig.height = 4.7, 
                      fig.width = 10, fig.retina = 3)
library(tidyverse)
```

---
## Nesse curso vamos falar de

TODO

---

## O que é Deep learning?

-   Subconjunto de técnicas de Machine Learning

![:scale 50%](img/artificial-intelligence.png)

Fonte: Deep Learning with R, Chollet, F et al

---
## O que é Deep learning?

- Aprender representações dos dados em camadas. Aprender representações dos dados 'hierarquicamente'. 

![:scale 75%](img/dl-deep.png)

Fonte: Deep Learning with R, Chollet, F et al

---

## Por que Deep Learning

-   Estado da arte em diversos problemas muito importantes.

![:scale 65%](img/Screen Shot 2020-05-09 at 10.51.43.png)

Fonte: [Apr. resultados ILSVRC 2017](http://image-net.org/challenges/talks_2017/ILSVRC2017_overview.pdf)

---

## Por que Deep learning?

- Resultados impressionantes em tradução.

![](img/image00.png)

Fonte: [A Neural Network for Machine Translation, at Production Scale](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)
---

## Por que Deep Learning?

- Resultados em Reinforcement Learning

![:scale 75%](img/unnamed.gif)

Fonte: [AlphaZero: Shedding new light on chess, shogi, and Go](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go)

---

# Por que Deep Learning?

- Reconhecimento de fala
- Reconhecimento em vídeos

![:scale 50%](img/1_vAhhl1cMGTzBUTYnIMssIQ.png)

Fonte: [Tesla’s Deep Learning at Scale: Using Billions of Miles to Train Neural Networks](https://towardsdatascience.com/teslas-deep-learning-at-scale-7eed85b235d3)

---

class: middle, center, inverse

# Tudo começa com ....

## Regressão Linear

---

## Regressão Linear

Prevendo o preço de uma casa a partir do tamanho em metros quadrados.

```{r}
df <- tibble(
  m2 = rgamma(n = 100, scale = 5, shape = 20),
  preco = 100000 + (6200 + rnorm(n = 100, sd = 800)) * m2 
)

ggplot(df, aes(x = m2, y = preco)) +
  geom_point() +
  coord_cartesian(ylim = c(0, NA)) +
  scale_y_continuous(
    name = "Preço (em R$)",
    labels = scales::dollar_format(prefix = "R$", big.mark = ".", decimal.mark = ",")
    
    ) +
  scale_x_continuous(name = "Área (em mˆ2)") +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x)
```

---

## Definição do modelo

Definimos o modelo de regressão linear da seguinte forma:

$$\hat{y_i} = w \times x_i + b$$
- $y_i$: preço do imóvel $i$
- $x_i$: área do imóvel $i$

Poderíamos escrever

$$\hat{y_i} = f(x_i)$$

em que:

- $f(x) = w \times x + b$

Chamamos $f$ de **'layer'** (camada) na linguagem do Deep learning.

---

## Layers (Camadas)

- Uma **'layer'** é uma transformação dos dados que é
parametrizada por **pesos**. 

- 'Aprender', então, significa encontrar os melhores **pesos** para cada camada.

No exemplo:

$$f(x) = w \times x + b$$

Os pesos são $w$ e $b$.

- Essas camadas são os 'tijolos' do Deep Learning e existem diversas 'camadas'.

- A camada do exemplo é chamada de **'Densa'** ou **'Linear'**.

- Um modelo pode possuir uma ou mais dessas camadas.

---

# Regressão Linear

![:scale 65%](img/Screen Shot 2020-05-09 at 14.32.20.png)

Fonte: Figura adaptada do Deep Learning with R, Chollet, F. et al.


**Objetivo**: encontrar os melhores 'pesos' para essa Layer.

---

## Função de perda

- Mede quanto o modelo está perto do que queremos que ele fique.

- No nosso caso, mede o quanto a previsão dada por $w \times x + b$ está perto de $y$, o verdadeiro valor daquele imóvel.

- Uma função de perda bastante usada é o **MSE** - Erro quadrático médio.

- O MSE é dado por:

$$L(\hat{y}) = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y_i})^2$$
- Podemos reescrever em função dos pesos:

$$L(w, b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - w \times x_i - b)^2$$
---

## Função de perda em função do valor dos parâmetros

```{r}
mse <- function(w, b) {
  mean((df$preco - (df$m2*w + b))^2)
}

expand.grid(
  w = seq(from = -10000, to = 10000, length.out = 300),
  b = seq(from = -200000, to = 200000, length.out = 100)
) %>% 
  rowwise() %>% 
  mutate(mse = mse(w, b)) %>% 
  ggplot(aes(x = w, y = b, fill = mse)) +
  geom_bin2d(stat = "identity") +
  scale_fill_gradient(high = "blue", low = "white", trans = "log") +
  theme_minimal() +
  geom_point(
    data = data.frame(w = 6200, b = 100000),
    aes(x = w, y = b, fill = NULL),
    color = "red",
    size = 5
  )
```

---

## Função de perda

![:scale 50%](img/Screen Shot 2020-05-09 at 18.30.44.png)

Fonte: Figura adaptada do Deep Learning with R, Chollet, F. et al.

- A **função de perda** nos diz, dado um valor para cada peso, o quão próximo estamos do valor esperado.

---

## Encontrando o mínimo da função de perda

Até agora:

- Vimos que nosso objetivo é minimizar a função de perda.
- Para isso precisamos encontrar o valor dos pesos que minimiza faz a função de perda ter o valor mínimo possível.

Agora:

- O processo de encontrar o mínimo de uma função é chamado de **otimização**. 
- Existem diversos algoritmos de otimização. Em geral eles são adequados ou não dependendo da função que você está otimizando.
- Em Deep Learning usamos algoritmos que são variações do **Gradient Descent** - método de descida do gradiente.

---

## Gradient Descent 

O **gradient descent** diz que se uma função $L(x)$ é diferenciável na vizinhança de um ponto $w$ então $L(x)$ decresce mais rapidamente se você andar de $w$ para uma direção contrária ao gradient de $L$ no ponto $w$. 

Em outras palavras, fazer $w - \nabla L(w)$ é a forma de caminhar o mais rápido possível para o mínimo de $L(x)$.

![:scale 20%](img/700px-Gradient_descent.svg.png)

Fonte: [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)

---

## Gradient descent no exemplo

No nosso exemplo temos: 

$$L(w, b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - w \times x_i - b)^2$$

Então para andar mais rápido para o mínimo de $L$ cada passo de $w$ e $b$ tem que ser calculado da seguinte forma:

$$w_{(k+1)} = w_{(k)} - \alpha \frac{\partial L}{\partial w} = w_{(k)} - \alpha \frac{1}{n}\sum_{i=1}^{n} (-2 \times x_i)(y_i - w \times x_i -b)$$

$$b_{(k+1)} = b_{(k)} - \alpha \frac{\partial L}{\partial b} = b_{(k)} - \alpha \frac{1}{n} \sum_{i=1}^{n} (-2)(y_i - w \times x_i -b)$$

**Calma!** vamos dissecar essas fórmulas.

---


## Otimizando

Esse é o diagrama geral que vale para os modelos que vamos implementar neste curso.

![:scale 50%](img/Screen Shot 2020-05-09 at 19.17.36.png)

Fonte: Figura adaptada do Deep Learning with R, Chollet, F. et al.

---

## Grafo de computação

.pull-left[

- É útil representar modelo em um grafo de computação.
- **Cinza** são os pesos.
- **Verde** são os dados.
- **Laranja** são variáveis derivadas de dados & pesos.

]

.pull-right[
![:scale 100%](img/Screen Shot 2020-05-09 at 22.13.42.png)
]


---

## Calculando as derivadas

.pull-left[

1. Calculamos as derivadas parciais para cada transformação.
2. Usamos a regra da cadeia para calcular as derivadas $\frac{\partial L}{\partial w}$ e $\frac{\partial L}{\partial b}$.

Pela regra da cadeia temos:

$$\frac{\partial L}{\partial w} = 2(y_i - \hat{y}) \times x_i$$

$$\frac{\partial L}{\partial b} = 2(y_i - \hat{y}) \times 1$$

**Nota**: tiramos as médias p/ simplicar a notação.

]

.pull-right[
![:scale 100%](img/Screen Shot 2020-05-09 at 22.13.21.png)
]

---

## Gradient descent

No fim temos que a regra de atualização dos pesos é:

$$w_{(k+1)} = w_{(k)} - \alpha \times \frac{1}{n} \sum_{i = 1}^n \left( 2(y_i - \hat{y}) \times x_i \right)$$

$$b_{(k+1)} = b_{(k)} - \alpha \times \frac{1}{n} \sum_{i = 1}^n \left( 2(y_i - \hat{y}) \times 1 \right)$$
- $\alpha$ é um hiper parâmetro que chamamos de **'learning rate'**. Ele controla com qual intensidade vamos andar na direção do gradiente.

- Na fórmula vemos que podemos obter $w_{(i+1)}$  em função do $w_{(i)}$, mas e o $w_{(0)}$? Geralmente inicializamos ele com algum número aleatório. A mesma coisa para o $b_{(0)}$.

---

## Gradient descent

- **Esquerda** eixos $b$ e $m$ representam $b$ e $w$ no nosso exemplo. O eixo 'Error' representa o valor da função de perda.

- Conseguimos visualizar a descida até o mínimo da função de perda pelo método do gradiente. 

- Na **direita** a reta ajustada para os dados.

![:scale 60%](img/0_D7zG46WrdKx54pbU.gif)

Fonte: [https://alykhantejani.github.io/images/gradient_descent_line_graph.gif](https://alykhantejani.github.io/images/gradient_descent_line_graph.gif)

---

class: center, middle

exemplo 01: exemplos/01-linear-regression.R

---

## Exercício 1

Arquivo: exercicios/01-linear-regression.R

![:scale 50%](img/Screen Shot 2020-05-11 at 14.25.07.png)

---

## SGD (Stochastic gradient descent)

- Em vez de calcular a média da derivada em todos os exemplos da base de dados, calculamos em apenas 1 e já andamos. Isto é, trocamos:

$$w_{(k+1)} = w_{(k)} - \alpha \times \frac{1}{n} \sum_{i = 1}^n \left( 2(y_i - \hat{y}) \times x_i \right)$$

por

$$w_{(k+1)} = w_{(k)} - \alpha \times \left( 2(y_i - \hat{y}) \times x_i \right)$$

- Cada vez que atravessamos a base inteira dessa forma chamamos de **'epoch'**.

- Agora é possível atualizar os pesos sem precisar fazer contas na base inteira. Mais **rápido**.

- Como estimamos o passo com apenas uma observação, os passos (principalmente quando já estão perto do mínimo) podem ser meio ruins.

---

## SGD

- Na prática, parece que o fato dos passos serem ruins perto do mínimo é bom - isso faz um certo tipo de regularização. Não se sabe explicar esse comportamento muito bem ainda.

![:scale 40%](img/Screen Shot 2020-05-11 at 14.39.43.png)

Fonte: https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/stochastic-gd.pdf

---

## Mini-batch SGD

- Em cada iteração, selecionamos uma amostra de tamanho $b$ da base de tamanho $n$. Em geral $b << n$. 

- Calculamos o passo do GD com essa amostra. No final temos:

$$w_{(k+1)} = w_{(k)} - \alpha \times \frac{1}{b} \sum_{i = 1}^b \left( 2(y_i - \hat{y}) \times x_i \right)$$

- Na prática é o que funciona melhor. Compensa o passo do SGD ser meio ruim por se basear em apenas uma observação e o fato do GD ser muito pesado computacionalmente para bases muito grandes.

- Em geral usa-se $b$ (**batch size**) múltiplos de 2.

---

## Mini-batch SGD

.pull-left[
![:scale 80%](img/Screen Shot 2020-05-11 at 14.48.37.png)
]

.pull-right[
![:scale 80%](img/Screen Shot 2020-05-11 at 14.48.44.png)
]

Fonte: https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/stochastic-gd.pdf

---

class: middle, center

exemplo 02: exemplos/02-sgd.R

---

class: middle, center

exercicio 02: exercicios/02-mini-batch-sgd.R

---

## TensorFlow

- Biblioteca open-source para cálculos numéricos.
- Desenvolvida incialmente pela Google.
- Foco em Machine Learning e principalmente Deep Learning.
- Muito rápido - implementado para diversos hardwares como GPU's e até TPU's.
- Feature: **Automatic Differentiation**!
- Um grande ecossistema de addosn e extensões.
- Biblioteca mais utilizada para fazer Deep Learning atualmente.

![:scale 75%](img/tf-logo.jpg)

---

## Keras

- É uma biblioteca open-source criada para especificar modelos de Deep Learning
- Foi criada antes do TensorFlow existir
- Funciona com múltiplos 'backends' - exemplo Theano, CNTK e PlaidML
- Foi **incorporada pelo TensorFlow** e à partir do 2.0 é a forma recomendada de
especificar modelos no TensorFlow

![](img/keras-logo-2018-large-1200.png)

---

## TensorFlow e Keras no R

- TensorFlow e Keras são [implementados no R usando o 'reticulate'](https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r/), isso significa que chamamos funções do Python. 

- Keras para o R fornece uma API muito mais *user fiendly* para quem já programa em R e quer aprender Deep Learning.

- Muitos guias e tutoriais [aqui](https://tensorflow.rstudio.com/).

- A performance é comparável com a do Python - as contas pesadas acontecem no C++. 

- A comunidade é menor -> tem seus prós e contras.


---

Fazer no R - exemplo 03